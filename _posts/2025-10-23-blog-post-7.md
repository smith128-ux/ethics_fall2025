---
title: 'BIAS Blog 1: Right to Fair Representation'
date: 2025-10-23
permalink: /posts/2025/10/blog-post-7/
tags:
  - Bias
  - BIAS blog post 1
  - Ethics
---
**Case Study**  
[AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia](https://mit-serc.pubpub.org/pub/bfw5tscj/release/3?readingCollection=65a1a268)

This week, I read the case study “AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia” about the ways that western centered development on AI T2I (Text to Image) models have resulted in disproportionately poor representation of South-Asian cultures, whereas the European and American centered prompts generate issues with no issue of depth or detail. I think the main issue lies in the disconnect between software development and ethical thinking. The western concept of foreign countries is often clouded by ignorance and inaccurate portrayals from the media, the way that Breaking Bad slathers a deep yellow filter over scenes meant to take place in South America. To gain a better understanding of the topic at hand, I wanted to respond to the six discussion questions provided at the end of the reading. 

What does cultural representation mean to you, and how might this definition and your experience with past representation impact how you would evaluate representation of your identity in generative AI models? What aspects of your identity do you think you would center when evaluating representations in AI model output?
I think accurate cultural representation should be varied in its output. I believe that a culture is accurately portrayed when several aspects of the culture, infrastructure, religion, and population can be expressed. I’m talking about articles of clothing, sexuality, arts, varied religious practices, daily life, etc. Representing a culture, only using images, can be difficult. It’s important though for these T2I images to not perpetuate harmful stereotypes about other cultures, especially since they have little to no issues representing western culture and daily life.

What do you think is the role of small-scale qualitative evaluations for building more ethical generative AI models? How do they compare to larger, quantitative, benchmark-style evaluations?
Small-scale qualitative evaluations like the ones conducted in this case study are perfect ways to measure the shortcomings and successes in ethical responses from generative AI models. With a smaller team of participants, individual quandaries and insights are more easily heard, giving the response more weight and depth. I think large evaluations often miss the finer details needed when tweaking models of such titanic size. With continuous small-scale evaluations of the ethics of T2I models, I believe we can make real headway with correcting these imbalances.

Participants in this study shared “aspirations” for generative AI to be more inclusive, but also noted the tensions around making it more inclusive. Do you think AI can be made more globally inclusive?
I believe the unfortunate reality of AI development is that it’s not always built with inclusivity in mind. There’s definitely an end goal with the big AI companies, and that goal doesn’t usually have the wellbeing of people in mind. I think we’re starting to see the consequences of computer science being largely dominated by white males, as this majority is often ignorant to its own ignorance. I believe that there are steps we can take to further improve inclusivity, but I don’t see a future where companies are willing to compromise. This kind of work to improve representation of non-western cultures takes a lot of time, time which I think companies would rather skip to expedite the deployment of newer, shinier T2I AI models.

What mitigations do you think developers could explore to respond to some of the concerns raised by participants in this study?
I never really considered how the frontier of AI development being based in the States would result in models with this ignorant, ill-informed, western bias on South-Asian cultures. I completely understand where these participants are coming from when they say they feel misrepresented by T2I depictions of their daily-life and feel like their own identities are at risk. I grew up in a half-Russian, half-American household, and was exposed to Russian culture growing up. During middle school, Russia has become sort of a “Meme” and the average Russian was being grossly misrepresented and mischaracterized. If you are a developer creating a T2I model, you are responsible for whatever it may output. Holding developers accountable for the ways their models misrepresent non-western cultures could be a good way to mitigate already prevalent issues in T2I images.

As mentioned in this case study, representation is not static; it changes over time, is culturally situated, and varies greatly from person to person. How can we “encode” this contextual and changing nature of representation into models, datasets, and algorithms? Is the idea of encoding at odds with the dynamism and fluidity of representation?
I would hope, like representation, these models don’t remain static. I would like to see continuous updating and learning from these models, taking in new up-to-date data. Companies developing these T2I models are already lacking in cultural knowledge. Since this knowledge is vast and everchanging, and these developers are obviously preoccupied with other motives, it may be up to these developers to code their T2I to keep itself in check. I think that we’re reaching a point where we’re developing models that are a lot like children. At first, we babysit them, train them, teach them “right” and “wrong”, and then eventually push them out into the real world. I see a future where AI models can update and revise themselves, without outsider influence, to better represent the current world as it stands.

How can we learn from the history of technology and media to build more responsible and representative AI models?
It’s usually been the case that if a group can recognize a gap in knowledge, then the solution is to bring on an expert in whatever field they may be lacking knowledge in. I think of how movies depicting alien planets and landscapes will bring on real biologists and experts knowledgeable on the inner workings of Earth's own ecosystem to create a more realistic and in-depth fictional world. I believe that the same concept of bringing in experts should be applied to these training T2I models to better represent non-western cultures. The extent to which cultures will be represented may vary, and there may not even be any “experts” in whatever culture they may be lacking context and information for. I do believe that if these massive, billion dollar companies can afford the millions of dollars that it takes in compute power to train their models, then they can afford to create several, robust and varied groups of cultural experts to better represent non-western countries and cultures. It may also be of interest to create a division of people who flag potentially harmful content that may misrepresent cultures with an already scarce amount of representation.

Inspired by these eloquently articulated discussion question, I would like to provide my own question:

As image models have begun to advance, they’ve also started to be trained on previously AI generated images. With AI models producing images lacking in inclusivity of global cultures, what are the consequences of training models on these images?

I ask this question because of the trend of AI images having this permanent, yellow-ish, “Ghibli” tint to it, because of the trend in people asking models to produce “Ghibli”-fication’s of images. The problem is, image models have started to be trained on AI generated images, resulting in a permanent yellow tinting, even on images not Ghibli-themed at all. If T2I algorithms are already displaying most south-Asian cultures as impoverished and dirty, will it only continue to get worse? Will the images they produce be even worse than the worse living conditions of those countries? Will T2I models continue to misrepresent and whitewash non-white characters?

I loved reading this case study! The findings of this case study provided a lot of insights on the consequences of misrepresentation and how isolating that can make somebody feel. I never even considered the consequences of computer science being a straight-white-male dominated field could have on the outputs of AI models. It makes a lot of sense. I hope that with continued diversity and research, computer science will bloom into a field with great diversity and variety in cultures and backgrounds.