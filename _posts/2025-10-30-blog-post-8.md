---
title: 'BIAS Blog 2: Harms in Machine Learning'
date: 2025-10-30
permalink: /posts/2025/10/blog-post-8/
tags:
  - Bias
  - BIAS blog post 2
  - Ethics
---
**Case Study**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

This week I read “Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle” by members of MIT’s Department of Computer Science and Artificial Intelligence Laboratory, Harini Suresh and John Guttag. This case study looked to debunk the pervasive idea that AI can be used as a non-biased decision maker, by highlighting the many aspects of a models development process that lead to bias pervasively entering into the design. This case study introduces Machine Learning, a basic overview of developing a model using Machine Learning, and seven sources of harmful biases that can arise during this development process.

Discussion questions and activities:
Can you come up with an additional example of each source of harm?
Historical Bias:
A recent example of the consequences of historical bias is with the recent “increase” of autism diagnoses. The truth is, the increase in autism is primarily because the definition of what is classified as autism has changed in recent years, and there's now early screenings for autism for young children. 
Representation Bias:
I would describe representation bias as stereotyping people based on attributions. For example, a lot of people will make the assumption that you're smart or like to study if you have glasses. On the other end of the spectrum, if you’re creating a model that writes tickets based on data of people pulled over and the resulting ticket, it’s probably gonna be pretty skewed towards minority groups.

Measurement Bias:
Measurement bias is the overestimation or underestimation of data. Due to the phenomena of placebo, if you were to have a test group and control group, and you were testing a new drug, the test group might report feeling different even if there's no noticeable changes, because they’re aware that they are in the test group. This leaves the data being poorly measurable and abstract.
Aggregation Bias:
Another example of aggregation bias would be creating a ML algorithm that detects COVID-19 based on the symptoms but never testing it on people with allergies, or with other underlying issues. This could result in an inaccurate model that might misinterpret a person's preexisting condition as a symptom of COVID-19.

Learning Bias:
Let’s say that I’m creating a ML algorithm to place kids in advanced classes based on data that tracks their behavior day-to-day. Let’s say there’s a bad kid named “Little Timmy” and Timmy’s teacher has it out for him. They only seem to notice when Timmy is doing something he’s not supposed to and ignores his actions when he does the right thing or is behaving appropriately. This would result in a ML algorithm that also exhibits these same biases towards Timmy, most likely resulting in him being placed in non-advanced classes.
Evaluation Bias:
If I'm deciding whether or not a potential candidate for a job gets hired, based on an algorithm that checks applications for keywords I’m looking for, there’s a lot of potential for Evaluation Bias. Maybe I favor good communication skills and a candidate has those listed as traits, but maybe the rest of their application isn’t well thought out or even finished. If I’m only evaluating candidates on those specific traits I could be ignoring obvious mistakes.

Deployment Bias:
Deployment bias is the inappropriate and harmful misuse of an algorithm, despite its technical accuracy, to amplify already prevalent social inequalities. The COMPAS model is a model that assesses the likelihood of a criminal becoming a repeat offender. The misuse of this model comes from judges using this model to determine prison sentence length, whether or not that person really was going to reoffend. This model also frequently has racial bias, and this sentencing only added to that disparity. 

Can you think of other sources of harm that aren’t captured in this case study?
Confirmation bias is an incredibly prevalent form of harmful bias in machine learning. Confirmation bias is when a person's initial beliefs influence the way that they interpret data or take action. Favoring evidence that confirms initial hypothesis without considering contradictory evidence. If you’re creating a model like COMPAS, and you already have preexisting racial bias towards minority groups, you may not see the results of your model as being harmful. You may take it as a fact, not considering the issue lies in the data or the training, etc.

Draft a checklist that people starting new projects could use to try to anticipate or prevent different types of bias.
What’s our model's purpose? Is our team composed of diverse individuals? Is the decision-making skewed or equal? What are our assumptions? (If applicable) Does our data contain any bias? Is it relevant? Are we using fair criteria for decision-making? Are we testing on diverse sets of data? Are we using inclusive language? Have we defined our metrics for fairness? Are we reviewing for blindspots? Do we have a clear process for receiving feedback? Have we considered the ethical outcomes of deployment? Have we established accountability? 
I believe with most/all of these criteria met, developers will be able to prevent bias from creeping into their ML algorithms.

Think about a specific ML-based project or product (something that you’ve worked on, used, or are familiar with) and think about how you might conduct each step of the data collection, development, and deployment process while being conscious of potential sources of harm.
Let’s say, for example, I’m creating a facial recognition technology and I’m aware of the common pitfalls most facial recognition models run into, a lack of diverse training. To avoid facial recognition that favors one group of people in particular, I might try to train my model on a dataset that contains a large variety of different races and ages of people. During development, I would test extensively to determine the success rate of facial recognition between the various groups of people. Once I’m sure that I’ve created a fairly unbiased facial recognition model, I would deploy it with caution. Perhaps because my model can recognize people of color better than most models, people might take advantage of that after deployment if they meant to do harm.

In response to these questions, I'd like to provide my own question, which I find important:
Do you think a developer is responsible for the misuse of their program? If so, to what extent?

I find this question interesting because what do we do if a developer does everything in their power to create a model with as little bias as possible, and people still find a way to cause harm using it after its deployment? Should we punish negligent developers for creating potentially harmful programs? How can we even gauge that? 

This case study was a wonderful read, reminding me a lot of my time spent studying machine learning last semester. I like the way that the case study introduces ML, then moves onto the steps that go into developing ML, and then explains seven examples of potentially harmful bias that could be introduced during development. Everything tied together very well and the conclusion brought it to a nice end. I don’t know if I myself will go into machine learning when I’m out of college, but It’s definitely a topic that interests me.

Thank you, I hope you enjoyed : - )
-Tony
