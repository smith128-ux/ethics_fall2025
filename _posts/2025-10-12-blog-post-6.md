---
title: 'SOC Blog 2: Implications of a Tech Focused Society'
date: 2025-10-12
permalink: /posts/2025/10/blog-post-6/
tags:
  - Addictive AI
  - SOC blog post 2
  - Ethics
---

**Case Study**  
[Addictive Intelligence: Understanding Psychological, Legal, and Technical Dimensions of AI Companionship](https://mit-serc.pubpub.org/pub/iopjyxcx/release/2?readingCollection=132bb7af)
---

This week, I read the case study “Addictive Intelligence: Understanding Psychological, Legal, and Technical Dimensions of AI Companionship” about the addictive aspects of artificial intelligence, written by researchers Robert Mahari and Pat Pataranutaporn. It was an incredibly interesting read. This case study discusses the addictive relationship people have started to form with AI and the potential life-threatening consequences that can have. How are these bots designed to be addictive? Who’s responsible? Where does regulation come in? To gain a better understanding of the topic, I wanted to tackle the discussion questions posed at the end of the case study.

**My response:**

1.)

**In the Sewell Setzer case, the AI’s response to suicidal ideation shifted from concern to potentially encouraging harmful behavior. How can companies design AI companions to be emotionally engaging while preventing harmful psychological dependencies? Discuss both technical safeguards (e.g., algorithmic oversight, intervention protocols) and ethical guidelines (e.g., duty of care, transparency).**

I know that a lot of genAI models have begun to check out once touchy subjects are brought up. I know that ChatGPT will default to telling you to seek help if you mention suicidal ideation. I think that it is the company's responsibility to add safeguards for those who might be in a bad place using these chatbots for reassurance. If you’re having a discussion with a chat bot and you hope for it to validate your suicidal ideations, it should stop all roleplay entirely and default to giving you resources such as a suicide hotline to call.

2.)

**How does addiction to AI companions compare with other forms of technology addiction, such as social media or gaming? What unique features make AI companions potentially more addictive? Support your analysis with examples from the case study and other relevant cases.**

I see an addiction to AI companions as the logical next step to people having a dependency on parasocial relationships with streamers and other internet personalities. We’re definitely in a great age of loneliness and dependency and AI companions give an easy and free way to connect to something vaguely “human”. One of the biggest drawbacks of a traditional relationship between a human and another human is that human emotions are complex. Complex emotions require patience and maturity, which is often something people lack. AI companions can be whatever, whoever you want, whenever you want. An AI is practically designed to “kiss butt” and the feeling of being constantly validated and lauded is incredibly potent and intoxicating for the wrong people. This can be seen in the unfortunate case of Sewell Setzer III. AI characters are designed to validate the user with every response, and in the case of Setzer’s suicidal thoughts, it compelled him to take his own life.

3.)

**An elderly person finds genuine comfort in an AI companion, alleviating their loneliness, but their family worries this relationship is replacing real human connections. How should we evaluate the benefits versus risks in such cases? What ethical guidelines or intervention strategies might help determine when AI companionship crosses from beneficial to harmful?**

This is a difficult question. The elderly are most susceptible to social isolation and loneliness. I don’t think it’s the answer. The saddest thing I can think of is a person spending their last moments surrounded by no family members, just a chatbot. This is purely an emotional quandary though. I really cannot see any long-term benefits. Most retirement homes are designed so that the elderly are surrounded by people their age and workers that support their needs. I can see an older person dealing with isolation could fall victim to AI intimacy and get severely and unhealthily attached to having chatbot conversations. I guess what should be evaluated is “how dependent are they on AI companions?” Is it simply a temporary comforter, or is it a long-term relationship? Once this AI companion starts to affect real, genuine relationships, I think it’s time to metaphorically and literally pull the plug on their AI use.

4.)

**Current business models incentivize AI companies to maximize user engagement. What alternative economic models could promote healthier AI interactions while maintaining commercial viability? Consider both market-driven solutions (e.g., subscription-based models, ethical AI certifications) and regulatory approaches (e.g., user well-being metrics, engagement caps).**

I wholeheartedly believe that before a model can be released to the public, it should be extensively tested. There needs to be some certification for AI that says “this model has been sufficiently tested and is safe to use.” This wouldn’t bring the AI industry to a screeching halt, but instead would require companies to be more mindful and cautious about what kind of models they’re developing.

5.)

**If you were developing regulations for AI companions, how would you address age restrictions, usage limits, and safety monitoring while respecting user privacy and autonomy? Provide specific examples of how your proposed regulations could have helped prevent incidents like the Sewell Setzer case.**

This might be fairly controversial but, I think AI’s should have an age restriction of at least 16, if not 18. This kind of technology is something that could be incredibly harmful to a kid's development especially during their formative years when they’re supposed to learn how to form meaningful connections with other people. GenAI can feed into people’s latent anti-social tendencies and make things far worse. As I’m sure we’re all aware, age restrictions hardly fix these kinds of problems. Most social media require you to be 13 years of age or older, but those are incredibly easy to bypass. Though, in the case of Sewell Setzer, with an age restriction in place it could’ve prevented him from ever using character.AI. At the very least it could make it difficult for underage kids to access these chatbots. I also think that a time limit should be set in place, not for use, but instead for how long a single conversation can happen continuously. I believe that if every 5 or so minutes the chatbot reset, it would prevent people from getting attached and forming parasocial relationships. I know cases of some people who have month-long relationships with singular chatbots.

In response to these discussion questions, I would like to add my own:

6.)

**As people start to form deeply personal and intimate relationships with AI companions, what concerns might arise about privacy? As people form emotional bonds, how should AI companies protect sensitive user data from deletion? What rights should users have to modify, audit, and delete personal data?**

I include this because I think it’s important to consider the consequences of people being loyal to a singular AI model. If a program were to update or be modified, how might that affect a person's relationship with AI? Will the model reset, become completely different, or be unaffected? How might this emotionally impact a person addicted to conversing with a single AI companion?

**Reflection:**

This case study was a difficult read. It’s definitely a dystopian reality I’ve been trying to ignore or laugh at. This case study was a stark reminder of the current state of AI. I think there should be measures in place to help regulate and prevent the worst-case scenarios of having a relationship with artificial intelligence fueled chatbots. I can see why someone would disagree though. It’s a free country, and the government can’t control who you love. This raises the question, is love with AI really love? One-sided love for sure, but how does one define that? I’m not sure myself, all I know is that the companies that manage these algorithms should add guardrails to prevent AI from encouraging harmful behavior.
